{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting Diabetes Occurrence in Pima Indians\n\nIn this notebook, we will be using the Pima Indians Diabetes dataset to build a model that can predict, given the factors, whether a given Pima Indian develops diabetes. The large number of missing values in the dataset will be filled in using **stochastic regression imputation**. \nOwing to the **class imbalance**, we use the precision and recall metrics to get a clearer picture of the model's performance. \n\nThese concepts will be explained in more detail in the relevant sections, and links to pages where you can learn more will be provided at the bottom.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Checking out the data**","metadata":{}},{"cell_type":"code","source":"full_dataset = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\n#Dataset accessible at https://www.kaggle.com/uciml/pima-indians-diabetes-database\n\nprint(\"Shape of dataset: \" + str(full_dataset.shape) + \"\\n\")\nprint(\"Number of zero outcomes (Did not develop diabetes): \" + str(len(full_dataset[full_dataset[\"Outcome\"]==0]))+ \"\\n\")\nprint(\"Number of one outcomes (Developed diabetes): \" + str(len(full_dataset[full_dataset[\"Outcome\"]==1]))+ \"\\n\")\n\npd.options.display.width = 0\n\nprint(full_dataset.head())\n\nprint(\"\\nStats: \\n\")\n\nprint(full_dataset.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that of all 768 records, only 268 actually developed diabetes - about a third of the dataset. This represents a **class imbalance**. Class imbalances can be a reason for misleadingly high accuracy score - for example, if we produce a model for this dataset which only predicts an outcome of zero, its accuracy will be 65%.\n\nNote also that some zero values are present where the feature logically cannot be a zero. For example, a person cannot have zero insulin, or they would be dead. The same goes for blood pressure, skin thickness, and BMI. We'll find out, in the first and second code cells below, how many items in each column are zero or NaN.\n\nWe will fill these zero values by using some method of data imputation. ","metadata":{}},{"cell_type":"code","source":"(full_dataset==0).sum(axis=0)  #this shows how many values are zero in each column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(pd.isna(full_dataset)==True).sum(axis=0)  #this shows how many values are nan in each column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the data","metadata":{}},{"cell_type":"markdown","source":"You will notice that while there are a few zero values in some fields - 11 for BMI, 35 for blood pressure, and 5 for glucose - there are many zero values for skin thickness and Insulin (227 and 374 respectively). These numbers represent a large fraction of the dataset, and therefore what we do with them carries more weight.\n\nLet us use a simple approach to fill the BMI, BloodPressure and Glucose columns - we will calculate the mean of those values throughout the netire dataset, and replace the zero values with the corresponding mean.","metadata":{}},{"cell_type":"code","source":"def replace_with_mean(df,feature):\n    \n    feature_mean = df[feature].mean()\n    df[feature] = list(map(lambda x: feature_mean if x==0 else x, df[feature]))\n\n\nreplace_with_mean(full_dataset,\"BloodPressure\")\nreplace_with_mean(full_dataset,\"BMI\")\nreplace_with_mean(full_dataset,\"Glucose\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can run this code cell to verify that there are no more zero values in these 3 columns.","metadata":{}},{"cell_type":"code","source":"(full_dataset==0).sum(axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dealing with the Insulin and SkinThickness values will be more complicated. If we simply fill all the missing values with their mean values computed from the same number of values, that reduces the variance of the data. There will be too many people all having the exact same value of insulin or skin thickness. This fools the model into getting the wrong idea of how much effect those features have on the outcome. \n\n\nAn approach we can take is to use data from similar cases to estimate a replacement value for the missing feature.\n\nTo get an idea of how to choose similar cases, let's plot a correlation matrix. This shows the relationship between every pair of features in the dataset. \n\nTo plot this correlation matrix, we will have to eliminate all records with those zero values. Let's do that on a copy of the full dataset.\n","metadata":{}},{"cell_type":"code","source":"modified = full_dataset[(full_dataset[\"Insulin\"]!=0) & (full_dataset[\"SkinThickness\"]!=0)]\n(modified==0).sum(axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrMatrix = modified.corr()\nsns.heatmap(corrMatrix,annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\nHigh correlation between skin thickness and BMI, and between insulin and glucose.\n\nSo we'll try to fill in the missing values using those. Let's make scatterplots to get an idea of how exactly they are related.","metadata":{}},{"cell_type":"code","source":"AX = sns.regplot(x=\"SkinThickness\",y=\"BMI\",data=modified)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AX = sns.regplot(x=\"Glucose\",y=\"Insulin\",data=modified)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By eyeballing these two plots, you can tell that linear regression with a little random error added would be a good way to fill in the missing skin thickness values. We don't want to go with the exact values that will be lying on the line shown, as that would also give a false sense of uniformity. \n\nYou can tell that the majority of the data points lie in a certain region around the line, something like a +- 10 region for BMI, and +-50 for insulin. So we can add a random number from that range to the predicted value. \n\nThat is what **stochastic regression imputation** is. \n\nLet's build models to predict insulin and skin thickness based on glucose and BMI respectively. \n\nWe'll be using scikit-learn's inbuilt LinearRegression model to do this. ","metadata":{}},{"cell_type":"code","source":"BMI = modified[\"BMI\"].to_numpy().reshape((-1,1))  #reshaping the dataframe in a way that the LinearRegression model can accept\nSkinT = modified[\"SkinThickness\"].to_numpy()\nskint_model = LinearRegression().fit(BMI,SkinT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Running a test prediction! The double set of square brackets is because the model expects a 2D array containing the data\n\nskint_model.predict([[40]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's do the same for Insulin.","metadata":{}},{"cell_type":"code","source":"Glucose = modified[\"Glucose\"].to_numpy().reshape((-1,1))\nInsulin = modified[\"Insulin\"].to_numpy()\ninsulin_model = LinearRegression().fit(Glucose,Insulin)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insulin_model.predict([[100]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now to replace the zero values of Insulin and Skin Thickness with the predicted values. We'll write a function that takes the dataset, names of the features, the trained model, and the error range as arguments, and replaces the relevant zero values with the values obtained by getting the model's prediction and adding a random error.","metadata":{}},{"cell_type":"code","source":"def replace(df,model,x,y,error_range):\n    #fill the missing values of the feature y, using values predicted from its x value.\n    \n    reshaped = df[x].to_numpy().reshape((-1,1))   #reshaping before feeding to the LinearRegression model.\n    y_pred = model.predict(reshaped)              #values predicted to be on the line\n    \n    random_err_array = np.random.randint(low=-1*error_range,high=error_range,size=len(y_pred))\n    #generating an array of random integers within the given error range\n    \n    y_pred += random_err_array                    #adding the random errors\n    \n    for i in range(len(df[y])):\n        if df[y][i]==0:\n            df[y][i] = y_pred[i]                  #replacing the zero values in the dataset.\n            \n\n\nreplace(full_dataset,insulin_model,\"Glucose\",\"Insulin\",50)\nreplace(full_dataset,skint_model,\"BMI\",\"SkinThickness\",10)\nfull_dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(full_dataset==0).sum(axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling the data and removing outliers (Normalization)","metadata":{}},{"cell_type":"markdown","source":"It's important that all the data be in roughly the same range. If not, the model may assign a disproportionately higher importance to features with larger absolute values. Scaling the features to the same range also allows gradient descent to converge faster. \n\nA popular method of scaling data is to use the Z-score. That's what we do in the following cell.","metadata":{}},{"cell_type":"code","source":"full_mean = full_dataset.mean()\nfull_std = full_dataset.std()\n\nfull_norm = (full_dataset-full_mean)/full_std\n\nfull_norm[\"Outcome\"] = full_dataset[\"Outcome\"] \n#we don't want the outcome column to be scaled, so we replace it with the original\nfull_norm.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking a look at the data to check for any remaining outliers. We generally expect Z-scores to lie between -3 and +3.","metadata":{}},{"cell_type":"code","source":"sns.distplot(full_norm[\"Pregnancies\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(full_norm[\"Insulin\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Turns out there are some large positive outliers. Let's put a cap on the values in each column - if any value is greater than 3, it gets replaced by 3. This won't affect the Outcome column, as its values are all 0 or 1. \n\nAfter running the below cell, you can re-run the two cells above to observe any changes.","metadata":{}},{"cell_type":"code","source":"for col in full_norm.columns:\n    full_norm[col] = list(map(lambda x: min(x,3),full_norm[col]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are finally done with normalizing the data!","metadata":{}},{"cell_type":"markdown","source":"# Preparing the model","metadata":{}},{"cell_type":"markdown","source":"We start by shuffling the data and splitting into train and test sets. ","metadata":{}},{"cell_type":"code","source":"final_shuffled = full_norm.sample(frac=1).reset_index(drop=True)  \n#this fully shuffles the dataset to avoid any sort of ordering being recognized by the model as a trainable quality\n\nfinal_shuffled.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(final_shuffled, test_size=0.2)\n#model will be run on the train set w a validation split of 0.2\n#test will be used to evaluate the model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x = train.iloc[:,0:8]\ntrain_y = train.iloc[:,8]\n\ntest_x = test.iloc[:,0:8]\ntest_y = test.iloc[:,8]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Defining plotting functions**\n\nThis plotting function will take the history and metrics of a model, and plot their change over the successive epochs as a graph. This is a better way to visualize how the model learns, than reading through lines of verbose output.","metadata":{}},{"cell_type":"code","source":"def plot_curve(epochs, history, list_of_metrics):  \n\n  plt.figure()\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Value\")\n\n  for m in list_of_metrics:\n    x = history[m]\n    plt.plot(epochs[1:], x[1:], label=m)\n\n  plt.legend()\n\nprint(\"Defined the function which plots the learning curve.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating the model**","metadata":{}},{"cell_type":"markdown","source":"We're using a simple Sequential model with one output neuron. Sigmoid activation is used for the neuron, so that we get a probability as an output. i.e. - a prediction of 0.65 would mean that the model gives the particular scenario 65% probability of being a diabetes patient.","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(units=1, input_shape=(8,),activation=tf.sigmoid),)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here comes the classification threshold and precision and recall metrics, which I mentioned at the beginning of the notebook.\n\n**Classification threshold** - the point above which the output probability is classified as a 1. Intuitively, we tend to use 0.5 as the threshold. But, depending on the data, especially in instances of class imbalance like we have here, changing up the threshold and seeing how it affects the metrics will help us tune the model. We'll be starting with a 0.5 here, and changing it up a bit.\n\n**Precision** - As I mentioned earlier, accuracy isn't the greatest metric in situations with a class imbalance. In binary classification problems, the precision metric will tell you the ratio of correctly predicted positives to total predicted positives. For this particular problem, precision is the percentage of times the model is correct when it predicts that a particular individual has diabetes.\n\n**Recall** - The recall metric tells you the ratio of correctly predicted positives to total actual positives. For this problem, it's the percentage of times the model predicts diabetes in people who actually have diabetes. \n\n\nLet's say our model is being used to warn people that they might have diabetes, and to get tested. We would want to make sure we warned all of the people who actually had diabetes, right? If we warn a few people who don't actually have diabetes, that's fine. But we want to make sure that everyone who actually has diabetes is warned. \n\nIn other words, we want a high recall. But be careful - high recall usually leads to lower precision, so keep an eye on the precision metric to make sure it's not dropping too much.","metadata":{}},{"cell_type":"code","source":"classification_threshold = 0.5\n\nMETRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n      tf.keras.metrics.Precision(name='precision',thresholds=classification_threshold),\n      tf.keras.metrics.Recall(name='recall',thresholds=classification_threshold)\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='Adam',loss=tf.keras.losses.BinaryCrossentropy(),metrics=METRICS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_x,train_y,batch_size=10,epochs=50,validation_split=0.2, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = history.epoch\nhist = pd.DataFrame(history.history)\nplot_curve(epochs,hist,['accuracy','precision','recall','loss'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_x,test_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that the recall metric isn't doing well during either the training or the testing. Let's lower the threshold to 0.35 and see how it looks.","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(units=1, input_shape=(8,),activation=tf.sigmoid),)\n\nclassification_threshold = 0.35\n\nMETRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n      tf.keras.metrics.Precision(name='precision',thresholds=classification_threshold),\n      tf.keras.metrics.Recall(name='recall',thresholds=classification_threshold)\n]\n\nmodel.compile(optimizer='Adam',loss=tf.keras.losses.BinaryCrossentropy(),metrics=METRICS)\nhistory = model.fit(train_x,train_y,batch_size=10,epochs=50,validation_split=0.2,verbose=0)\nepochs = history.epoch\nhist = pd.DataFrame(history.history)\nplot_curve(epochs,hist,['accuracy','precision','recall','loss'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This graph looks much better. Let's evaluate the model on the test set.","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_x,test_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that the recall is higher during training than during testing?\n\nThis sounds like overfitting. Let's introduce some regularization. \n\nRegularization works by preventing the model's weights from continually rising in an attempt to reach zero loss. A penalty term which estimates the complexity of the model is added to the loss. For logistic regression, L2 regularization is often used. ","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(units=1, input_shape=(8,),\n                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.2),\n                                  activation=tf.sigmoid),)\n\nclassification_threshold = 0.35\n\nMETRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n      tf.keras.metrics.Precision(name='precision',thresholds=classification_threshold),\n      tf.keras.metrics.Recall(name='recall',thresholds=classification_threshold)\n]\n\nmodel.compile(optimizer='Adam',loss=tf.keras.losses.BinaryCrossentropy(),metrics=METRICS)\nhistory = model.fit(train_x,train_y,batch_size=10,epochs=50,validation_split=0.2,verbose =0)\nepochs = history.epoch\nhist = pd.DataFrame(history.history)\nplot_curve(epochs,hist,['accuracy','precision','recall','loss'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_x,test_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We finally have a higher recall than before! It's not perfect, but it looks pretty good. Our precision is also pretty good-looking.\n\nFeel free to copy and edit this notebook, and play around with the classification threshold and L1 and L2 values to see how much more you can optimize these metrics. Make sure you're training a fresh model from scratch each time. You can edit the values in the single cell above and run it repeatedly. \n\nHappy training!","metadata":{}},{"cell_type":"markdown","source":"***References :***\n\nData imputation - an overview of the methods used:\n\nhttps://www.theanalysisfactor.com/seven-ways-to-make-up-data-common-methods-to-imputing-missing-data/\n\nWhy mean imputation isn't always a good idea:\n\nhttps://www.theanalysisfactor.com/mean-imputation/\n\nThe effect of outliers and how scaling with Z-scores works:\n\nhttps://developers.google.com/machine-learning/crash-course/representation/cleaning-data\n\nPrecision and recall metrics:\n\nhttps://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n\nL2 regularization and the Lambda value:\n\nhttps://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization\nhttps://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda\nhttps://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n","metadata":{}}]}